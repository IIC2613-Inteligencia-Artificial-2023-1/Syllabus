# **Ayudantía Redes Neuronales**
## Tabla de Contenidos
* [Importando Librerías y Cargando Datos](#ch1)
* [Creando una Red Neuronal](#ch2)
* [Optimizaciones/Modificaciones](#ch3)
    * [Early Stopping](#ch31)
    * [Dropout](#ch32)
    * [Pruning](#ch33)
* [Creando una Red Neuronal Sin Utilizar Keras](#ch4)
## Importando Librerías y Cargando Datos <a class="anchor" id="ch1"></a>
# Importamos librerías de utilidad
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Importamos tensorflow
import tensorflow as tf
# Tensorflow viene con un dataset de MNIST. Cargamos el dataset
fashion_mnist = tf.keras.datasets.fashion_mnist

# Ahora lo cargamos en 4 arrays
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# Veamos las dimensiones del dataset
print(f"\nLa base de datos de entrenamiento tiene {train_images.shape[0]} imágenes de {train_images.shape[1]} x {train_images.shape[2]}")
print(f"\nLa base de datos de testeo tiene {test_images.shape[0]} imágenes de {test_images.shape[1]} x {test_images.shape[2]}")
# A modo de ejemplo, veamos la primera imagen del dataset (es una bota)
plt.imshow(train_images[0], cmap='gray')
# Por convenienca, normalizamos las imágenes. Esto ayuda a que los pesos de la red no tomen valores muy grandes (es decir, en un rango muy amplio)
train_images = train_images / 255.0
test_images = test_images / 255.0
# Si quisieramos, podemos tomar una porción de los datos de entrenamiento para usarlos como validación
# Esto nos permite ver cómo se comporta la red en datos que no fueron usados para entrenarla

# Tomamos las primeras 5.000 imágenes
X_valid = train_images[:5000]

# Ahora debemos dejar la base de entrenamiento sin esas 5.000 imágenes
X_train = train_images[5000:]

# Hacemos lo mismo para la variable respuesta
y_valid = train_labels[:5000]
y_train = train_labels[5000:]
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Como las imágenes son de 28x28, las podemos aplanar y usarlas directamente como vectores input de la red
train_images = train_images.reshape(np.shape(train_images)[0], 784)
test_images = test_images.reshape(np.shape(test_images)[0], 784)

# Veamos las nuevas dimensiones
print(np.shape(train_images))
## Creando una Red Neuronal <a class="anchor" id="ch2"></a>
# Hay muchas maneras de construir una red neuronal. Por ahora, vamos a usar la clase Sequential de Keras

model = tf.keras.Sequential([
# En caso de no aplanar las imágenes previamente, podemos usar la siguiente línea
tf.keras.layers.Flatten(input_shape=(28, 28)),
# Ahora agregamos las capas ocultas
tf.keras.layers.Dense(50, activation='relu'),
tf.keras.layers.Dense(100, activation='relu'),
tf.keras.layers.Dense(500, activation='relu'),
tf.keras.layers.Dense(10)
])


# También podemos definir una función auxiliar para compilar una red
def compile_model(n_layers, n_neurons, activation, optimizer, metrics):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
    for i in range(n_layers):
        model.add(tf.keras.layers.Dense(n_neurons[i], activation=activation))
    model.add(tf.keras.layers.Dense(10))
    model.compile(optimizer=optimizer,
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=metrics)
    return model
# Ahora, para compilar el modelo debemos especificar el optimizador, la función de pérdida y las métricas que queremos usar para entrenar la red
model.compile(optimizer='adam',
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
            metrics=['accuracy'])
# Por ahora, utilizaremos batches para entrenar más rápido la red
# Definimos el tamaño de cada batch
batch_size = 256
# Definimos la cantidad de épocas de entrenamiento
epochs = 30

# Guardamos el historial del entrenamiento
history = model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = epochs, batch_size = batch_size)
# Creamos una función de ayuda para extraer la información guardada en el objeto history
def plot_history(history):

    # Hacemos una grilla de 2 figuras
    fig, axs = plt.subplots(2, figsize = (10, 10))

    # Graficamos los accuracy tanto el set de entrenamiento como el de validación
    axs[0].plot(history.history["accuracy"], label = "Accuracy entrenamiento")
    axs[0].plot(history.history["val_accuracy"], label = "Accuracy validación")
    axs[0].set_ylabel("Accuracy")
    axs[0].legend(loc = "lower right")
    axs[0].set_title("Accuracy por épocas")

    # De la misma manera, graficamos el error tanto para el set de entrenamiento como el de validación
    axs[1].plot(history.history["loss"], label="Error entrenamiento")
    axs[1].plot(history.history["val_loss"], label="Error validación")
    axs[1].set_ylabel("Error")
    axs[1].set_xlabel("Época")
    axs[1].legend(loc = "upper right")
    axs[1].set_title("Error por épocas")

    # Mostramos la figura 
    plt.show()
plot_history(history)
Intentemos mejorar el accuracy agregando más capas ocultas con más neuronas
model = compile_model(4, (100,200,200,100), "relu", "adam", ["accuracy"])

# Definimos el tamaño de cada batch
batch_size = 256

# Definimos la cantidad de épocas de entrenamiento
epochs = 30

# Ajustamos el modelo
history = model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = epochs, batch_size = batch_size)
plot_history(history)
Vemos que estamos alcanzando un mejor accuracy al agregar más neuronas. 

Sin embargo, ¿serán todos los parámetros necesarios? Es decir, ¿podemos encapsular la misma información aprendida pero con menos parámetros? Como vimos, al tener menos neuronas pareciera que el accuracy no es tan bueno. Una posible solución es realizar algún tipo de poda. Esto lo veremos un poco después (más abajo).
# Optimizaciones/Modificaciones <a class="anchor" id="ch3"></a>
¿Qué pasa si entrenamos sin batches? ¿Tomará mucho tiempo?
model = compile_model(4, (100,200,200,100), "relu", "adam", ["accuracy"])

model.fit(
    X_train,
    y_train,
    validation_data = (X_valid, y_valid),
    epochs=30
)
Efectivamente toma mucho tiempo (en comparación a entrenar con batches). Además, vemos que los accuracies se empiezan a estancar (tal como lo vimos en los gráficos anteriores).

Una solución es parar el entrenamiento luego de una cierta cantidad de épocas en donde el modelo no está mejorando significativamente.
## Early Stopping <a class="anchor" id="ch31"></a>
# Importamos el callback que usaremos
from tensorflow.keras.callbacks import EarlyStopping

# Definimos el EarlyStopping
early_stopping = EarlyStopping(patience = 3, monitor = "val_accuracy", restore_best_weights = True)

# Volvemos a compilar el primer modelo
model = compile_model(4, (100,200,200,100), "relu", "adam", ["accuracy"])

# Definimos el tamaño de cada batch si es que lo quieren usar (recomendado)
#batch_size = 256

# Definimos la cantidad de épocas de entrenamiento
epochs = 30

# Agregamos callbacks = [early_stopping] al método fit para que detecte cuando se estanca el entrenamiento
history = model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = epochs, callbacks = [early_stopping])
Vemos que paró el entrenamiento apenas en la sexta época. Esto definitivamente ahorra tiempo, pero observemos el gráfico
plot_history(history)
Ahora, ¿qué podemos hacer si nos importa mucho que nuestra red sea robusta ante datos nuevos y que no ocurra overfitting? Una posible solución es podar neuronas y/o parámetros. Veamos esto durante el entrenamiento (Dropout) y después del entrenamiento (Pruning).
## Dropout <a class="anchor" id="ch32"></a>
Dropout permite quitar un cierto porcentaje de NEURONAS aleatoriamente con cierta probabilidad en cada iteración.

<img src="https://miro.medium.com/max/1044/1*iWQzxhVlvadk6VAJjsgXgg.png">
model = tf.keras.Sequential([
tf.keras.layers.Flatten(input_shape=(28, 28)),
tf.keras.layers.Dense(100, activation='relu'),
# Esta es una capa de Dropout
tf.keras.layers.Dropout(0.4),
tf.keras.layers.Dense(200, activation='relu'),
tf.keras.layers.Dropout(0.4),
tf.keras.layers.Dense(200, activation='relu'),
tf.keras.layers.Dropout(0.4),
tf.keras.layers.Dense(100, activation='relu'),
tf.keras.layers.Dropout(0.4),
tf.keras.layers.Dense(10)
])

model.compile(optimizer='adam',
            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
            metrics=['accuracy'])

batch_size = 256
epochs = 30
history = model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = epochs, batch_size=batch_size)
plot_history(history)
# Pruning <a class="anchor" id="ch33"></a>
La idea detrás de esto es entrenar un modelo con todas las neuronas que queramos, pero luego guardamos solo los parámetros que codifican información. Por ejemplo, y como lo haremos a continuación, aquellos que estén sobre algún valor threshold. En otras palabras, los parámetros que tomen valores muy pequeños los truncamos a 0 simplemente.

La gracia de esto es que almacenar matrices sparse (recordemos que este tipo de redes neuronales se pueden representar como matrices) es más eficiente (solo almacenamos las entradas distintas a cero), entre muchos otros beneficios. 
# Importamos la librería que nos ayuda a hacer pruning. Es una librería que no viene incluida en tensorflow mismo y hay que instalarla
import tensorflow_model_optimization as tfmot

# Definimos la función que aplica el pruning a una capa densa
def apply_pruning_to_dense_40(layer):
  pruning_params = {
      'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(
          target_sparsity=0.4,#qué porcentaje de pesos queremos podar
          begin_step=1, #en qué paso empezar a podar
          end_step=-1, #en qué paso terminar de podar (-1 es al final)
          frequency=5 #cada "frequency" pasos deberíamos podar (no son epochs)
      )
}
  if isinstance(layer, tf.keras.layers.Dense):
    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)# "prune_low_magnitude" es el método que aplica el pruning
  return layer
base_model = compile_model(4, (100,200,200,100), "relu", "adam", ["accuracy"])
history = base_model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = 30, batch_size = 256)
plot_history(history)
model = compile_model(4, (100,200,200,100), "relu", "adam", ["accuracy"])

loss, acc = model.evaluate(X_train, y_train, verbose=2)

# Clonamos el modelo pero utilizamos la función de pruning en el proceso
model_for_pruning = tf.keras.models.clone_model(
    base_model,
    clone_function=apply_pruning_to_dense_40,
)

model_for_pruning.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Aquí también usamos callbacks, pero en este caso es para actualizar los pesos
callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]

# Entrenamos el modelo igual que antes
pruning_history = model_for_pruning.fit(
    X_train,
    y_train,
    validation_data = (X_valid, y_valid),
    callbacks = callbacks,
    epochs=30
)


plot_history(pruning_history)
# Definimos una función para extraer los pesos de la red
def get_params(model, n_layers):
    w = []
    b = []
    # En este caso, nos saltamos la priemra capa porque es la de aplanamiento
    for i in range(1,n_layers+1):
        w.append(model.layers[i].get_weights()[0].T)
        b.append(model.layers[i].get_weights()[1].T)

    return w, b
# Revisemos que el pruning efectivamente funcionó

# Primero, consigamos los parámetros de la red sin pruning
w_np, b_np = get_params(base_model, 4)
# Contemos la cantidad de parámetros no nulos
for i in range(1,5):
    w_i = w_np[i-1]
    sparsity = 1 - np.count_nonzero(w_i)/float(w_i.size)
    print(f"Sparsity of layer {i} for original model: {sparsity}")

# Repitamos el proceso para la red con pruning
w_p, b_p = get_params(model_for_pruning, 4)
for i in range(1,5):
    w_i = w_p[i-1]
    sparsity = 1 - np.count_nonzero(w_i)/float(w_i.size)
    print(f"Sparsity of layer {i} for pruned model: {sparsity}")
Es decir, tenemos que un 40% de los pesos de la red son 0, y alcanzamos un accuracy similar a la red sin pruning!
Si les interesa más del tema, les dejo un paper sobre este tema. La idea es que uno puede obtener, a partir de una red entrenada, una subred (con muy pocos parámetros) que obtiene rendimientos muy parecidos, o incluso mejores, a la red neuronal original. No es trivial cuándo esto ocurre, por lo que hay muchas investigaciones sobre el tema.

The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks (el paper original): https://arxiv.org/abs/1803.03635
## Creando una Red Neuronal Sin Utilizar Keras <a class="anchor" id="ch4"></a>
Si bien Keras es muy útil, podemos crear una red neuronal desde cero (casi) nosotros mismos. Los conceptos son los mismos que los vistos en clases.
class NeuralNetwork:
    def __init__(self, layers):
        self.layers = layers
        self.L = len(layers)
        self.num_features = layers[0]
        self.num_classes = layers[-1]
        # Guaramos nuetros weights y biases en diccionarios
        self.W = {}
        self.b = {}
        # Estos los utilizaremos para los gradientes
        self.dW = {}
        self.db = {}
        
        self.setup()
        
    def setup(self):
       # Inicializamos los parámetros de la red, utilizando una distribución normal para los parámetros
       # Esta inizialización importa bastante a la hora de construir una red. Otra alternativa es usar una distribución uniforme
        for i in range(1, self.L):
            self.W[i] = tf.Variable(tf.random.normal(shape=(self.layers[i],self.layers[i-1])))
            self.b[i] = tf.Variable(tf.random.normal(shape=(self.layers[i],1)))
    
    def forward_pass(self, X):
        # Esto es tal cual como se vio en clases
        A = tf.convert_to_tensor(X, dtype=tf.float64)
        for i in range(1, self.L):
            # Esto es tranformar el input de la capa en su output
            Z = tf.matmul(A,tf.transpose(self.W[i])) + tf.transpose(self.b[i])
            if i != self.L-1:
                # Aquí aplicamos la función de activación
                # Se puede cambiar por otra función de activación
                A = tf.nn.relu(Z)
            else:
                A = Z
        return A
    
    def compute_loss(self, A, Y):
        loss = tf.nn.softmax_cross_entropy_with_logits(Y,A)
        return tf.reduce_mean(loss)
    
    
    def update_params(self, lr):
        for i in range(1,self.L):
            self.W[i].assign_sub(lr * self.dW[i])
            self.b[i].assign_sub(lr * self.db[i])
    
    def predict(self, X):
        # Realizamos el forward pass para transformar el input en un output
        A = self.forward_pass(X)
        # Aplicamos la función de activación softmax para obtener las probabilidades y luego tomamos el mayor
        return tf.argmax(tf.nn.softmax(A), axis=1)
    
    def train_on_batch(self, X, Y, lr):
        X = tf.convert_to_tensor(X, dtype=tf.float64)
        Y = tf.convert_to_tensor(Y, dtype=tf.float64)
        # El GradientTape nos permite calcular los gradientes de los parámetros e ir guardando un historial de lo que hacemos
        with tf.GradientTape(persistent=True) as tape:
            A = self.forward_pass(X)
            loss = self.compute_loss(A, Y)
        for i in range(1, self.L):
            # Se utiliza diferenciación automática para calcular los gradientes
            self.dW[i] = tape.gradient(loss, self.W[i])
            self.db[i] = tape.gradient(loss, self.b[i])
        del tape
        self.update_params(lr)
        return loss.numpy()

    def train(self, x_train, y_train, x_test, y_test, epochs, steps_per_epoch, batch_size, lr):
        history = {
            'val_loss':[],
            'train_loss':[],
            'val_acc':[]
        }
        for e in range(0, epochs):
            epoch_train_loss = 0.
            print('Epoch {}'.format(e), end='.')
            for i in range(0, steps_per_epoch):
                x_batch = x_train[i*batch_size:(i+1)*batch_size]
                y_batch = y_train[i*batch_size:(i+1)*batch_size]
                
                batch_loss = self.train_on_batch(x_batch, y_batch,lr)
                epoch_train_loss += batch_loss
                
                if i%int(steps_per_epoch/10) == 0:
                    print(end='.')
                    
            history['train_loss'].append(epoch_train_loss/steps_per_epoch)
            val_A = self.forward_pass(x_test)
            val_loss = self.compute_loss(val_A, y_test).numpy()
            history['val_loss'].append(val_loss)
            val_preds = self.predict(x_test)
            val_acc =    np.mean(np.argmax(y_test, axis=1) == val_preds.numpy())
            history['val_acc'].append(val_acc)
            print('Validation accuracy:',val_acc)
        return history
from keras.utils import to_categorical

model = NeuralNetwork([784,128,128,10])

y_train_net = to_categorical(y_train)
y_valid_net = to_categorical(y_valid)

x_train_net = X_train.reshape(np.shape(X_train)[0], 784)
x_valid_net = X_valid.reshape(np.shape(X_valid)[0], 784)

batch_size = 120
epochs = 5
steps_per_epoch = int(train_images.shape[0]/batch_size)
lr = 3e-3

history = model.train(
    x_train_net,y_train_net,
    x_valid_net, y_valid_net,
    epochs, steps_per_epoch,
    batch_size, lr)
def plot_history(history):
    fig, axs = plt.subplots(2, figsize = (10, 10))
    axs[0].plot(history["val_acc"], label = "Accuracy validación")
    axs[0].set_ylabel("Accuracy")
    axs[0].legend(loc = "lower right")
    axs[0].set_title("Accuracy por épocas")
    axs[1].plot(history["val_loss"], label="Error validación")
    axs[1].set_ylabel("Error")
    axs[1].set_xlabel("Época")
    axs[1].legend(loc = "upper right")
    axs[1].set_title("Error por épocas")
    plt.show()

plot_history(history)
Pueden leer más en: https://ruslanmv.com/blog/Neural-Networks-in-Tensorflow
